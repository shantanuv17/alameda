hourlyPredict = false

[watchdog]
delayedSec = 120
  [watchdog.model]
  directory = "/tmp/model"
  [watchdog.predict]
  directory = "/tmp/predict"

[datahub]
address = "datahub.alameda.svc.cluster.local:50050"
connRetry = 5
  [datahub.query]
  retry = 3
  retryInterval = 10 # seconds

[queue]
url = "amqp://admin:adminpass@rabbitmq.alameda.svc.cluster.local:5672"
  [queue.retry]
  publishTime = 5
  publishIntervalMs = 3000
  consumeTime = 5
  consumeIntervalMs = 3000
  connectIntervalMs = 3000
  ackTimeoutSec = 3
  [queue.consumer]
  reconnectInterval = 30 #seconds

[serviceSetting]
granularities = ["30s", "1m", "1h", "6h", "24h"]
predictUnits = ["POD", "GPU", "NAMESPACE",
  "APPLICATION", "CLUSTER", "CONTROLLER", "NODE"
]
# must put NODE predict unit at last, because to send
# NODE jobs with granularity 30s depends on POD job
# with granularity 30s are sent

[granularities]

  [granularities.24h]
  dataGranularity = "24h"
  dataGranularitySec = 86400
  predictionSteps = 30
  predictionJobSendIntervalSec = 86400
  modelJobSendIntervalSec = 86400

  [granularities.6h]
  dataGranularity = "6h"
  dataGranularitySec = 21600
  predictionSteps = 30
  predictionJobSendIntervalSec = 21600
  modelJobSendIntervalSec = 21600

  [granularities.1h]
  dataGranularity = "1h"
  dataGranularitySec = 3600
  predictionSteps = 30
  predictionJobSendIntervalSec = 3600
  modelJobSendIntervalSec = 3600

  [granularities.1m]
  dataGranularity = "1m"
  dataGranularitySec = 60
  predictionSteps = 60
  predictionJobSendIntervalSec = 60
  modelJobSendIntervalSec = 60

  [granularities.30s]
  dataGranularity = "30s"
  dataGranularitySec = 30
  predictionSteps = 30
  predictionJobSendIntervalSec = 30
  modelJobSendIntervalSec = 30

[predictUnits]

  [predictUnits.POD]
  type = "POD"

  [predictUnits.NODE]
  type = "NODE"

  [predictUnits.GPU]
  type = "GPU"

  [predictUnits.NAMESPACE]
  type = "NAMESPACE"

  [predictUnits.APPLICATION]
  type = "APPLICATION"

  [predictUnits.CLUSTER]
  type = "CLUSTER"

  [predictUnits.CONTROLLER]
  type = "CONTROLLER"

[log]
setLogcallers = true
outputLevel = "info" # debug, info, warn, error, fatal, none

[model]
enabled = false
timeout = 180

[measurements]
  current = "mape"
  minimumDataPoints = 5
  maximumDataPoints = 5
  [measurements.mape]
  threshold = 15
  [measurements.rmse]
  threshold = 10
    [measurements.rmse.normalization]
    cpu = 1 #millicores
    memory = 1000000 #bytes
    dutyCycle = 0.2

# api proto metric type
[metricType]
undefined = 0
cpu_seconds_total = 1
cpu_millicores_total = 2
cpu_millicores_avail = 3
cpu_millicores_usage = 4
cpu_millicores_usage_pct = 5
cpu_millicores_allocatable = 6
memory_bytes_total = 7
memory_bytes_avail = 8
memory_bytes_usage = 9
memory_bytes_usage_pct = 10
memory_bytes_allocatable = 11
fs_bytes_total = 12
fs_bytes_avail = 13
fs_bytes_usage = 14
fs_bytes_usage_pct = 15
http_requests_count = 16
http_requests_total = 17
http_response_count = 18
http_response_total = 19
disk_io_seconds_total = 20
disk_io_utilization = 21
restarts_total = 22
power_usage_watts = 23
temperature_celsius = 24
duty_cycle = 25
current_offset = 26
lag = 27
latency = 28
number = 29

# api proto table
[scope]
undefined = 0
application = 1
metric = 2
planning = 3
prediction = 4
recommendation = 5
resource = 6

# api proto aggregation function
[aggregation]
none = 0
max = 1
avg = 2

[[units]]
enabled = true
scope = "application"
category = "kafka"
type = "topic"
measurement = "kafka_topic"
idKeys = ["cluster_name", "namespace", "name"]
granularities = ["1m"]
metricTypes = ["current_offset"]
predictor = "SARIMAX"
[units.valueKeys]
  scalerNamespace = "alameda_scaler_namespace"
  scalerName = "alameda_scaler_name"

  [units.metric]
  scope = "metric"
  category = "kafka"
  type = "topic"
  aggregation = "avg"
    [units.metric.valueKeys]
    value = "value"

  [units.prediction]
  scope = "prediction"
  category = "kafka"
  type = "topic"
    [units.prediction.valueKeys]
    modelID = "model_id"
    predictID = "prediction_id"
    granularity = "granularity"
    value = "value"


[[units]]
enabled = true
scope = "application"
category = "kafka"
type = "consumer_group"
measurement = "kafka_consumer_group"
idKeys = ["cluster_name", "namespace", "name", "topic_name"]
granularities = ["1m"]
metricTypes = ["current_offset"]
predictor = "SARIMAX"
[units.valueKeys]
  scalerNamespace = "alameda_scaler_namespace"
  scalerName = "alameda_scaler_name"
  resourceK8SNamespace = "resource_k8s_namespace"
  resourceK8SName = "resource_k8s_name"

  [units.metric]
  scope = "metric"
  category = "kafka"
  type = "consumer_group"
  aggregation = "avg"
    [units.metric.valueKeys]
    value = "value"

  [units.prediction]
  scope = "prediction"
  category = "kafka"
  type = "consumer_group"
    [units.prediction.valueKeys]
    modelID = "model_id"
    predictID = "prediction_id"
    granularity = "granularity"
    value = "value"